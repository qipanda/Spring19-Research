{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant modules\n",
    "%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from Models.SGNS import SourceReceiverConcatClassifier, SourceReceiverConcatModel\n",
    "from Preprocessing.FullContextProcessor import FullContextProcessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRC CV Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results = pd.read_csv(\"src-nicepaths-cv-results.txt\", sep=\"\\t\")\n",
    "cv_results.loc[:, [\"params\", \"mean_test_Log-Loss\"]].sort_values(by=\"mean_test_Log-Loss\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results.loc[16, \"params\"])\n",
    "print(cv_results.loc[34, \"params\"])\n",
    "print(cv_results.loc[5, \"params\"])\n",
    "print(cv_results.loc[29, \"params\"])\n",
    "print(cv_results.loc[23, \"params\"])\n",
    "print(cv_results.loc[11, \"params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trained SRC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mappings and original data\n",
    "fcp = FullContextProcessor(data_fpath=\"../Data/OConnor2013/ocon-nicepaths-extracted.txt\", sep=\"\\t\")\n",
    "\n",
    "# Create monthly time id's\n",
    "fcp.createMonthTimeIdx(colname=\"DATE\")\n",
    "\n",
    "# Create mappings\n",
    "fcp.createTwoWayMap(\"SOURCE\")\n",
    "fcp.createTwoWayMap(\"RECEIVER\")\n",
    "fcp.createTwoWayMap(\"WORD\")\n",
    "fcp.convertColToIdx(\"SOURCE\")\n",
    "fcp.convertColToIdx(\"RECEIVER\")\n",
    "fcp.convertColToIdx(\"WORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100,\n",
       "       101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
       "       114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "       127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152,\n",
       "       153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
       "       166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
       "       179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n",
       "       192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
       "       205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
       "       218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230,\n",
       "       231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243,\n",
       "       244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256,\n",
       "       257, 258, 259, 260, 261, 262, 263,   0,   1,   2,   3,   4,   5,\n",
       "         6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,\n",
       "        19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,\n",
       "        32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,\n",
       "        45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,\n",
       "        58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "        71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "        84,  85,  86,  87])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcp.df.t.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SourceReceiverConcatModel(s_cnt=len(fcp.df[\"SOURCE\"].unique()),\n",
    "                            r_cnt=len(fcp.df[\"RECEIVER\"].unique()),\n",
    "                            w_cnt=len(fcp.df[\"WORD\"].unique()),\n",
    "                            K_s=100,\n",
    "                            K_r=100,\n",
    "                            K_w=200)\n",
    "\n",
    "model.load_state_dict(torch.load(\"src-K200-lr1-wd1e-6-bs32.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the emebddings into numpy arrays\n",
    "s_embeds = model.s_embeds.weight.detach().numpy()\n",
    "r_embeds = model.r_embeds.weight.detach().numpy()\n",
    "w_embeds = model.w_embeds.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word and SR Tensorflow projection generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write word vectors out for tensorflow projector\n",
    "np.savetxt(fname=\"w_embeds.txt\",\n",
    "           X=w_embeds,\n",
    "           fmt=\"%.8f\",\n",
    "           delimiter=\"\\t\",)\n",
    "\n",
    "with open(\"w_labels.txt\", \"w\") as f:\n",
    "    for idx, w in fcp.twoway_maps[\"WORD\"][\"idx_to_col\"].items():\n",
    "        f.write(str(w) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write source vectors out for tensorflow projector\n",
    "np.savetxt(fname=\"s_embeds.txt\",\n",
    "           X=s_embeds,\n",
    "           fmt=\"%.8f\",\n",
    "           delimiter=\"\\t\",)\n",
    "\n",
    "with open(\"s_labels.txt\", \"w\") as f:\n",
    "    for idx, s in fcp.twoway_maps[\"SOURCE\"][\"idx_to_col\"].items():\n",
    "        f.write(str(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write source vectors out for tensorflow projector\n",
    "np.savetxt(fname=\"r_embeds.txt\",\n",
    "           X=r_embeds,\n",
    "           fmt=\"%.8f\",\n",
    "           delimiter=\"\\t\",)\n",
    "\n",
    "with open(\"r_labels.txt\", \"w\") as f:\n",
    "    for idx, s in fcp.twoway_maps[\"RECEIVER\"][\"idx_to_col\"].items():\n",
    "        f.write(str(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"sr_embeds.txt\", \"w\") as embeds_file, open(\"sr_labels.txt\", \"w\") as labels_file:\n",
    "    for (s, r), df in fcp.df.groupby([\"SOURCE\", \"RECEIVER\"]):\n",
    "        sr_embed = np.concatenate((s_embeds[s, :], r_embeds[r, :]))\n",
    "        embeds_file.write(\"\\t\".join([str(sr_val) for sr_val in sr_embed]) + \"\\n\")\n",
    "        labels_file.write(\n",
    "            fcp.twoway_maps[\"SOURCE\"][\"idx_to_col\"][s] + \\\n",
    "            \"-\" + \\\n",
    "            fcp.twoway_maps[\"RECEIVER\"][\"idx_to_col\"][r] + \"\\n\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicate Path Analysis per SR pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain pred path count rankings\n",
    "# Highest predicate_path counts per (s, r) will receive lowest numerical rank (e.g. highest count gets rank 1)\n",
    "# For each group of tied predicates, rank is the mean numerical rank among the group (e.g. if 3 things have the highest count, they all get rank (1+2+3)/3=2)\n",
    "sr_w_rankings = fcp.df.pivot_table(index=[\"SOURCE\", \"RECEIVER\"],\n",
    "                                   columns=\"WORD\",\n",
    "                                   fill_value=0,\n",
    "                                   aggfunc=\"size\").rank(axis=1,\n",
    "                                                        method=\"min\",\n",
    "                                                        ascending=False,\n",
    "                                                        pct=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_w_counts = fcp.df.pivot_table(index=[\"SOURCE\", \"RECEIVER\"],\n",
    "                                   columns=\"WORD\",\n",
    "                                   fill_value=0,\n",
    "                                   aggfunc=\"size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare for each (s,r) in data, most important pred_pathes dicated by model and simple count\n",
    "\n",
    "top_words = 5\n",
    "for (s, r), df in fcp.df.groupby([\"SOURCE\", \"RECEIVER\"]):\n",
    "    # Calc what model would consider as top words for given (s,r)\n",
    "    sr_embed = np.concatenate((s_embeds[s, :], r_embeds[r, :]))\n",
    "    sr_word_prod = np.dot(sr_embed, w_embeds.T)\n",
    "    top_model_words = [(word_idx, fcp.twoway_maps[\"WORD\"][\"idx_to_col\"][word_idx]) \n",
    "                        for word_idx in np.argsort(sr_word_prod)][-top_words:]\n",
    "    \n",
    "    # Calc top words by simple count\n",
    "    top_count_words = [fcp.twoway_maps[\"WORD\"][\"idx_to_col\"][word_idx] \n",
    "                       for word_idx in np.argsort(sr_w_counts.loc[(s, r)].values)[-top_words:]]    \n",
    "    \n",
    "    \n",
    "    print(fcp.twoway_maps[\"SOURCE\"][\"idx_to_col\"][s], fcp.twoway_maps[\"RECEIVER\"][\"idx_to_col\"][r], df.shape[0])\n",
    "    for i, ((word_idx, pred_path), count_word) in enumerate(zip(top_model_words, top_count_words)):\n",
    "        print(\"{}\\t{:.0f}\\t{} | {}\".format(top_words-i, sr_w_rankings.loc[(s, r)][word_idx], pred_path, count_word))\n",
    "    print(\"-\"*80)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation can be P(+|s,r,pred_path) = \"How likely was it that [s] [pred_path]'d [r] was reported in the news\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Find generalized model predictions for top pred_paths for (s,r)'s that don't exist (show usefulness of s, r embeddings)\n",
    "# sources = fcp.df[\"SOURCE\"].unique()\n",
    "# receivers = fcp.df[\"RECEIVER\"].unique()\n",
    "# sr_from_data = fcp.df.loc[:, [\"SOURCE\", \"RECEIVER\"]].values.tolist()\n",
    "\n",
    "# for s, r in np.array(np.meshgrid(sources, receivers)).T.reshape(-1,2):\n",
    "#     if [s, r] not in sr_from_data:\n",
    "#         # Calc what model would consider as top words for given (s,r) not in data\n",
    "#         sr_embed = np.concatenate((s_embeds[s, :], r_embeds[r, :]))\n",
    "#         sr_word_prod = np.dot(sr_embed, w_embeds.T)\n",
    "#         top_model_words = [(word_idx, fcp.twoway_maps[\"WORD\"][\"idx_to_col\"][word_idx]) \n",
    "#                             for word_idx in np.argsort(sr_word_prod)][-top_words:]\n",
    "        \n",
    "#         print(fcp.twoway_maps[\"SOURCE\"][\"idx_to_col\"][s], fcp.twoway_maps[\"RECEIVER\"][\"idx_to_col\"][r])\n",
    "#         for i, (word_idx, pred_path) in enumerate(top_model_words):\n",
    "#             print(\"{}\\t{}\".format(top_words-i, pred_path))\n",
    "#         print(\"-\"*80)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
